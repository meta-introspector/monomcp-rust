# CRQs and Insights from Project Work

## 1. CRQs/Tasks for Future Work

Here are some CRQs (Change Requests) and tasks that have emerged from our recent work, categorized for clarity and future planning:

### Category: `hf-dataset-validator` Enhancements

*   **Infrastructure: Integration of `apache/arrow-rs`**
    *   **Description:** The `apache/arrow-rs` library has been integrated as a vendored submodule (`monomcp/vendor/arrow-rs`). This provides the foundational Arrow data structures and Parquet file format support, which are crucial for the `hf-dataset-validator`'s ability to generate and process analysis data in `data.parquet` files.

*   **CRQ: Implement Comprehensive `DependencyAnalysis` in `cargo2hf_extractor`**
    *   **Description:** The `extract_dependency_analysis` function in `monomcp/vendor/hugging-face-dataset-validator-rust/src/cargo2hf_extractor.rs` has been implemented to extract detailed dependency information using `cargo metadata`. It now populates the `dependency_analysis-phase/data.parquet` files with comprehensive dependency data.
    *   **Tasks:**
        *   Investigate `cargo metadata` output structure for comprehensive dependency data.
        *   Map `cargo metadata` output to `CargoProjectRecord`'s dependency fields.
        *   Implement parsing and data extraction logic in `extract_dependency_analysis`.
        *   Write unit tests for `extract_dependency_analysis`.
*   **CRQ: Implement `SourceCodeAnalysis` in `cargo2hf_extractor`**
    *   **Description:** The `extract_source_code_analysis` function has been implemented to gather basic source code metrics like lines of code and file counts. It populates `source_code_analysis-phase/data.parquet` files. Further work is needed for detailed complexity analysis and item counts.
    *   **Tasks:**
        *   Research suitable Rust parsing/analysis libraries (e.g., `syn`, `rust-analyzer`'s crates, `tree-sitter`).
        *   Define metrics and their extraction methods.
        *   Implement data extraction and populate `CargoProjectRecord` fields.
        *   Write unit tests.
*   **CRQ: Implement `BuildAnalysis` in `cargo2hf_extractor`**
    *   **Description:** The `extract_build_analysis` function has been implemented to analyze build configurations, including the presence and lines of code of `build.rs` scripts, and extracts feature flags and targets from `Cargo.toml`. It populates `build_analysis-phase/data.parquet` files.
    *   **Tasks:**
        *   Understand `build.rs` execution and its impact on project metadata.
        *   Extract feature flag combinations and their usage.
        *   Analyze compilation profiles and settings.
        *   Write unit tests.
*   **CRQ: Implement `EcosystemAnalysis` in `cargo2hf_extractor`**
    *   **Description:** The `extract_ecosystem_analysis` function has been implemented to gather ecosystem data from `crates.io` (download counts) and GitHub (stars, forks, issues, last updated). It populates `ecosystem_analysis-phase/data.parquet` files.
    *   **Tasks:**
        *   Research `crates.io` API and GitHub API for relevant data.
        *   Implement API calls and data parsing.
        *   Populate `CargoProjectRecord` fields.
        *   Write unit tests.
*   **CRQ: Implement `VersionHistory` Analysis in `cargo2hf_extractor`**
    *   **Description:** The `extract_version_history` function has been implemented to analyze Git commit history, counting commits, unique contributors, and calculating project age. It populates `version_history-phase/data.parquet` files. Further work is needed for release frequency analysis.
    *   **Tasks:**
        *   Research `git2` crate for Git repository interaction.
        *   Implement Git history traversal and data extraction.
        *   Populate `CargoProjectRecord` fields.
        *   Write unit tests.
*   **Task: Address `parquet` and `pmat` warnings in `hf-dataset-validator`**
    *   **Description:** Investigate and resolve the remaining warnings generated by the `parquet` and `pmat` crates during `hf-dataset-validator` compilation. This might involve enabling features, updating dependencies, or submitting upstream fixes.

### Category: `project_reporter` Development

*   **CRQ: Enhance `project_reporter` to Display Full Analysis Data**
    *   **Description:** The `project_reporter` has been enhanced to read and display data from all generated Parquet files, including project metadata, lines of code, direct dependencies, build script presence, download counts, and commit counts. It now provides a more comprehensive overview of analyzed projects.
    *   **Tasks:**
        *   Update `project_reporter/src/main.rs` to read multiple Parquet files per project.
        *   Design a comprehensive table or multiple tables to display the richer data.
        *   Implement data aggregation and presentation logic.
*   **CRQ: Add Filtering and Sorting to `project_reporter`**
    *   **Description:** Implement command-line arguments for `project_reporter` to allow users to filter and sort the displayed project data (e.g., by version, description keywords, number of dependencies).
    *   **Tasks:**
        *   Integrate `clap` for command-line argument parsing.
        *   Implement filtering and sorting logic on the loaded data.
*   **CRQ: Export `project_reporter` Output to Different Formats**
    *   **Description:** Add functionality to `project_reporter` to export the generated table to other formats like CSV, JSON, or Markdown.

### Category: `project_reporter` Development

*   **CRQ: Enhance `project_reporter` to Display Full Analysis Data**
    *   **Description:** The `project_reporter` has been enhanced to read and display data from all generated Parquet files, including project metadata, lines of code, direct dependencies, build script presence, download counts, and commit counts. It now provides a more comprehensive overview of analyzed projects.
    *   **Tasks:**
        *   Update `project_reporter/src/main.rs` to read multiple Parquet files per project.
        *   Design a comprehensive table or multiple tables to display the richer data.
        *   Implement data aggregation and presentation logic.
*   **CRQ: Add Filtering and Sorting to `project_reporter`**
    *   **Description:** Implement command-line arguments for `project_reporter` to allow users to filter and sort the displayed project data (e.g., by version, description keywords, number of dependencies).
    *   **Tasks:**
        *   Integrate `clap` for command-line argument parsing.
        *   Implement filtering and sorting logic on the loaded data.
*   **CRQ: Export `project_reporter` Output to Different Formats**
    *   **Description:** Add functionality to `project_reporter` to export the generated table to other formats like CSV, JSON, or Markdown.

### Category: Fuzzing Infrastructure

*   **CRQ: Establish Dedicated Mermaid Fuzzer Crate**
    *   **Description:** Isolated Mermaid diagram fuzzer targets from `paiml-mcp-agent-toolkit/fuzz` into a new dedicated crate `monomcp/models/diagram/mermaid/fuzzing`. This resolved compilation issues related to `rustc-hash` version mismatches and provides a cleaner, isolated environment for fuzzing. A minimal reproduction crate (`monomcp/models/diagram/mermaid/hash_repro_crate`) was also created to diagnose the hash type mismatch.
    *   **Status:** Implemented and verified.

### General Project Updates

*   **Submodule Synchronization:** All project submodules have been synchronized with their respective `meta-introspector` forks, resolving local divergences and ensuring all changes are pushed upstream. This included re-adding `monomcp/simd-kernels` cleanly.

### Category: Cargo Integration (Long-Term Vision)

*   **CRQ: Integrate `cargo2hf_extractor` as a New Cargo Subcommand**
    *   **Description:** The `cargo2hf_extractor` has been integrated as a new Cargo subcommand, `cargo hf-export`. This allows direct execution of the extraction logic via `cargo hf-export <project_path>`. The `cargo` repository was previously vendored into `monomcp/vendor/cargo-rust-lang` to enable this integration.
    *   **Tasks:**
        *   Deep dive into Cargo's internal architecture and command dispatch.
        *   Design the new subcommand's interface and arguments.
        *   Copy/integrate `cargo2hf_extractor` code into Cargo's source tree.
        *   Adapt `cargo2hf_extractor` to use Cargo's internal APIs for data extraction.
        *   Address any new dependency conflicts or build issues.
        *   Implement comprehensive testing for the new subcommand.
        *   (Optional but highly recommended): Engage with the upstream Cargo team for potential future upstreaming.

---

## 2. Poem of Cargo Crates

(For social media inspiration)

In Rust's grand garden, where crates brightly gleam,
A builder's ambition, a vibrant new dream.
From `monomcp`'s heart, a vision takes flight,
To gather insights, and bring data to light.

`hf-validator`, a keen eye, now takes its stand,
To parse `Cargo.toml`s across the land.
With `arrow` and `parquet`, a data stream flows,
Though `snap`'s gentle feature, a small panic shows.

We patched and we prodded, with patience and might,
Through `workspace = true` in the code's endless night.
`cargo-util` and `schemas`, now tamed and aligned,
And `crates-io`'s secrets, we're starting to find.

No longer a "temp," but a `project_reporter` bold,
A table of knowledge, a story unfolds.
Each crate's metadata, a line, clear and bright,
Revealing its purpose, in pure textual light.

But deeper we yearn, for the tree's hidden root,
To see every dependency, bear every fruit.
For `cargo` itself, a new path we now chart,
To make it a data-exporting work of art.

So follow our journey, as Rust's power we wield,
Unlocking the codebase, a bountiful field.
From `Cargo.toml`'s whispers, to Parquet's grand might,
We're building the future, with data and light!
